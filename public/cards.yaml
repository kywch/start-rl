# Each card is identified by a unique keyword, followed by these fields:
#   title: The title of the card
#   tags: An array of tags to categorize the card
#   website: The website url
#   video: The youtube video url
#   paper: The paper/arXiv url
#   repo: The github repository url
#   notes: One-liner comment about the paper


GoogleColab:
  title: Google Colab Notebooks
  website: https://colab.research.google.com/
  notes: To use GPU, search how to use gpu in google colab.

RL-Iceberg:
  title: The Full Reinforcement Learning Iceberg
  video: https://www.youtube.com/watch?v=RIkse0tJ0hE
  notes: Dive into 10 levels of the RL stack with Joseph Suarez, a newly minted MIT PhD and the creator of Neural MMO + PufferLib. There's something here for beginners and world-class experts alike.

CleanRL:
  title: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms
  website: https://docs.cleanrl.dev/
  paper: https://arxiv.org/abs/2111.08819
  repo: https://github.com/vwxyzjn/cleanrl
  notes: Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch.

CleanRL-PPO:
  title: CleanRL Proximal Policy Optimization (PPO) single file implementation
  website: https://docs.cleanrl.dev/algorithms/ppo
  repo: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari.py

CleanRL-DQN:
  title: CleanRL Deep Q-Learning (DQN) single file implementation
  website: https://docs.cleanrl.dev/algorithms/dqn
  repo: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py

PufferLib:
  title: Making Reinforcement Learning Libraries and Environments Play Nice
  website: https://pufferai.github.io/
  paper: https://arxiv.org/abs/2406.12905
  repo: https://github.com/pufferai/pufferlib
  notes: Simplifying reinforcement learning for complex game environments. Ships with fast research-grade envs that runs 1M+ step/second on a gaming PC.

DQN:
  title: Human-level control through deep reinforcement learning
  paper: https://daiwk.github.io/assets/dqn.pdf
  notes: This paper started the deep learning revolution in reinforcement learning.

GAE:
  title: High-Dimensional Continuous Control Using Generalized Advantage Estimation
  paper: https://arxiv.org/abs/1506.02438
  notes: GAE improves policy gradient methods by optimizing advantage estimation, balancing bias and variance.

PPO:
  title: Proximal Policy Optimization Algorithms
  paper: https://arxiv.org/abs/1707.06347
  notes: PPO is a popular, simple, and fast RL algorithm that improves policy optimization by making small, controlled updates, balancing exploration and exploitation.

OpenAI-Five:
  title: Dota 2 with Large Scale Deep Reinforcement Learning
  website: https://openai.com/index/openai-five/
  paper: https://arxiv.org/abs/1912.06680
  notes: On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.

AlphaStar:
  title: Grandmaster level in StarCraft II using multi-agent reinforcement learning
  website: https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/
  paper: https://www.nature.com/articles/s41586-019-1724-z
  notes: AlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions.

EmergentToolUse:
  title: Emergent Tool Use From Multi-Agent Autocurricula
  website: https://openai.com/index/emergent-tool-use/
  paper: https://arxiv.org/abs/1909.07528
  notes: Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination.

CaptureTheFlag:
  title: Human-level performance in 3D multiplayer games with population-based reinforcement learning
  website: https://deepmind.google/discover/blog/capture-the-flag-the-emergence-of-complex-cooperative-agents/
  paper: https://www.science.org/doi/10.1126/science.aau6249
  notes: In Quake III Arena Capture the Flag, the trained agents successfully cooperate with both artificial and human teammates, and demonstrate high performance even when trained with reaction times comparable to human players.

XLand:
  title: Generally capable agents emerge from open-ended play
  website: https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/
  paper: https://arxiv.org/abs/2107.12808
  notes: We created a vast game environment we call XLand, which includes many multiplayer games within consistent, human-relatable 3D worlds. We find the agent exhibits general, heuristic behaviours such as experimentation, behaviours that are widely applicable to many tasks rather than specialised to an individual task.

NetHack:
  title: The NetHack Learning Environment
  website: https://ai.meta.com/blog/nethack-learning-environment-to-advance-deep-reinforcement-learning/
  paper: https://arxiv.org/abs/2006.13760
  repo: https://github.com/heiner/nle
  notes: The NetHack Learning Environment combines lightning-fast simulation with very complex game dynamics that are difficult even for humans to master. This allows our agents to experience billions of steps in the environment in a reasonable time frame while still challenging the limits of what current methods can achieve.