{
  "title": "New to RL? You can start now.",
  "texts": [
    {
      "id": 1729282475828,
      "content": "Start by watching",
      "x": 35,
      "y": 64,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 35
    },
    {
      "id": 1729282643261,
      "content": "A gaming pc with gpu would be nice, but if not use:",
      "x": 21,
      "y": 16,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 16,
      "zIndex": 26
    },
    {
      "id": 1729282994056,
      "content": "Like to see the working code?",
      "x": 36,
      "y": 118,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 37
    },
    {
      "id": 1729283067553,
      "content": "The modern classics:",
      "x": 31,
      "y": 305,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 42
    },
    {
      "id": 1729286993763,
      "content": "Awesome projects:",
      "x": 29,
      "y": 411,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 59
    }
  ],
  "cards": [
    {
      "key": "GoogleColab",
      "title": "Google Colab Notebooks",
      "website": "https://colab.research.google.com/",
      "notes": "To use GPU, search how to use gpu in google colab.",
      "x": 405,
      "y": 13,
      "zIndex": 27
    },
    {
      "key": "RL-Iceberg",
      "title": "The Full Reinforcement Learning Iceberg",
      "video": "https://www.youtube.com/watch?v=RIkse0tJ0hE",
      "notes": "Dive into 10 levels of the RL stack with Joseph Suarez, a newly minted MIT PhD and the creator of Neural MMO + PufferLib. There's something here for beginners and world-class experts alike.",
      "x": 247,
      "y": 63,
      "zIndex": 36
    },
    {
      "key": "CleanRL",
      "title": "High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms",
      "paper": "https://arxiv.org/abs/2111.08819",
      "website": "https://docs.cleanrl.dev/",
      "repo": "https://github.com/vwxyzjn/cleanrl",
      "notes": "Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch.",
      "x": 52,
      "y": 175,
      "zIndex": 38
    },
    {
      "key": "CleanRL-PPO",
      "title": "CleanRL Proximal Policy Optimization (PPO) single file implementation",
      "website": "https://docs.cleanrl.dev/algorithms/ppo",
      "repo": "https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari.py",
      "x": 205,
      "y": 174,
      "zIndex": 39
    },
    {
      "key": "CleanRL-DQN",
      "title": "CleanRL Deep Q-Learning (DQN) single file implementation",
      "website": "https://docs.cleanrl.dev/algorithms/dqn",
      "repo": "https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py",
      "x": 362,
      "y": 174,
      "zIndex": 40
    },
    {
      "key": "PufferLib",
      "title": "Making Reinforcement Learning Libraries and Environments Play Nice",
      "paper": "https://arxiv.org/abs/2406.12905",
      "website": "https://pufferai.github.io/",
      "repo": "https://github.com/pufferai/pufferlib",
      "notes": "Simplifying reinforcement learning for complex game environments. Ships with fast research-grade envs that runs 1M+ step/second on a gaming PC.",
      "x": 52,
      "y": 240,
      "zIndex": 41
    },
    {
      "key": "DQN",
      "title": "Human-level control through deep reinforcement learning",
      "paper": "https://daiwk.github.io/assets/dqn.pdf",
      "notes": "This paper started the deep learning revolution in reinforcement learning.",
      "x": 58,
      "y": 352,
      "zIndex": 53
    },
    {
      "key": "GAE",
      "title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
      "paper": "https://arxiv.org/abs/1506.02438",
      "notes": "GAE improves policy gradient methods by optimizing advantage estimation, balancing bias and variance.",
      "x": 217,
      "y": 352,
      "zIndex": 56
    },
    {
      "key": "PPO",
      "title": "Proximal Policy Optimization Algorithms",
      "paper": "https://arxiv.org/abs/1707.06347",
      "notes": "PPO is a popular, simple, and fast RL algorithm that improves policy optimization by making small, controlled updates, balancing exploration and exploitation.",
      "x": 370,
      "y": 353,
      "zIndex": 55
    },
    {
      "key": "OpenAI-Five",
      "title": "Dota 2 with Large Scale Deep Reinforcement Learning",
      "paper": "https://arxiv.org/abs/1912.06680",
      "website": "https://openai.com/index/openai-five/",
      "notes": "On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.",
      "x": 56,
      "y": 457,
      "zIndex": 69
    },
    {
      "key": "AlphaStar",
      "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning",
      "paper": "https://www.nature.com/articles/s41586-019-1724-z",
      "website": "https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/",
      "notes": "AlphaStar is the first AI to reach the top league of a widely popular esport without any game restrictions.",
      "x": 219,
      "y": 459,
      "zIndex": 80
    },
    {
      "key": "EmergentToolUse",
      "title": "Emergent Tool Use From Multi-Agent Autocurricula",
      "paper": "https://arxiv.org/abs/1909.07528",
      "website": "https://openai.com/index/emergent-tool-use/",
      "notes": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination.",
      "x": 379,
      "y": 460,
      "zIndex": 92
    },
    {
      "key": "CaptureTheFlag",
      "title": "Human-level performance in 3D multiplayer games with population-based reinforcement learning",
      "paper": "https://www.science.org/doi/10.1126/science.aau6249",
      "website": "https://deepmind.google/discover/blog/capture-the-flag-the-emergence-of-complex-cooperative-agents/",
      "notes": "In Quake III Arena Capture the Flag, the trained agents successfully cooperate with both artificial and human teammates, and demonstrate high performance even when trained with reaction times comparable to human players.",
      "x": 57,
      "y": 524,
      "zIndex": 108
    },
    {
      "key": "XLand",
      "title": "Generally capable agents emerge from open-ended play",
      "paper": "https://arxiv.org/abs/2107.12808",
      "website": "https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/",
      "notes": "We created a vast game environment we call XLand, which includes many multiplayer games within consistent, human-relatable 3D worlds. We find the agent exhibits general, heuristic behaviours such as experimentation, behaviours that are widely applicable to many tasks rather than specialised to an individual task.",
      "x": 220,
      "y": 523,
      "zIndex": 109
    },
    {
      "key": "NetHack",
      "title": "The NetHack Learning Environment",
      "paper": "https://arxiv.org/abs/2006.13760",
      "website": "https://ai.meta.com/blog/nethack-learning-environment-to-advance-deep-reinforcement-learning/",
      "repo": "https://github.com/heiner/nle",
      "notes": "The NetHack Learning Environment combines lightning-fast simulation with very complex game dynamics that are difficult even for humans to master. This allows our agents to experience billions of steps in the environment in a reasonable time frame while still challenging the limits of what current methods can achieve.",
      "x": 381,
      "y": 524,
      "zIndex": 110
    }
  ],
  "zoom": 1
}