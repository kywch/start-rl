{
  "title": "Awesome open-source robotics research",
  "texts": [
    {
      "id": 1729106979325,
      "content": "Benchmark",
      "x": 78,
      "y": 37.5,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "zIndex": 100,
      "fontSize": 24
    },
    {
      "id": 1729123066696,
      "content": "Policy/training",
      "x": 382.25,
      "y": 37.25,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 102
    },
    {
      "id": 1729123083640,
      "content": "Data collection/synthesis",
      "x": 693.2500000000001,
      "y": 30.25,
      "isEditing": false,
      "isSelected": false,
      "color": "#000000",
      "fontSize": 24,
      "zIndex": 103
    }
  ],
  "cards": [
    {
      "key": "ReKep",
      "title": "Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation",
      "tags": [
        "FeiFei",
        "policy",
        "hierarchical",
        "OmniGibson"
      ],
      "arxiv": "https://arxiv.org/abs/2409.01652",
      "website": "https://rekep-robot.github.io/",
      "github": "https://github.com/huangwl18/ReKep",
      "notes": "Large vision models and vision-language models can generate keypoint-based constraints, which can be optimized to achieve multi-stage, in-the-wild, bimanual, and reactive behaviors, without task-specific training or environment models.",
      "x": 405.5,
      "y": 212.25000000000003,
      "zIndex": 112
    },
    {
      "key": "RoboCasa",
      "title": "Large-Scale Simulation of Everyday Tasks for Generalist Robots",
      "tags": [
        "NVIDIA",
        "YukeZhu",
        "benchmark",
        "dataset",
        "RoboSuite"
      ],
      "arxiv": "https://arxiv.org/abs/2406.02523",
      "website": "https://robocasa.ai/",
      "github": "https://github.com/robocasa/robocasa",
      "notes": "2,500 3D assets across 150+ object categories and dozens of interactable furniture and appliances. A suite of 100 everyday activities with 100k demonstrations.",
      "x": 78.25,
      "y": 141.75,
      "zIndex": 107
    },
    {
      "key": "MimicGen",
      "title": "A Data Generation System for Scalable Robot Learning using Human Demonstrations",
      "tags": [
        "NVIDIA",
        "DieterFox",
        "YukeZhu",
        "dataset",
        "RoboSuite"
      ],
      "arxiv": "https://arxiv.org/abs/2310.17596",
      "website": "https://mimicgen.github.io/",
      "github": "https://github.com/NVlabs/mimicgen",
      "notes": "We used MimicGen to autonomously generate over 50,000 demonstrations from less than 200 human demonstrations across 18 tasks",
      "x": 690.4444444444443,
      "y": 145.36111111111103,
      "zIndex": 110
    },
    {
      "key": "RoboMimic",
      "title": "What Matters in Learning from Offline Human Demonstrations for Robot Manipulation",
      "tags": [
        "YukuZhu",
        "FeiFei",
        "policy",
        "RoboSuite"
      ],
      "arxiv": "https://arxiv.org/pdf/2108.03298",
      "website": "https://robomimic.github.io/",
      "github": "https://github.com/ARISE-Initiative/robomimic",
      "notes": "A Framework for Robot Learning from Demonstration",
      "x": 333.5,
      "y": 143.75000000000003,
      "zIndex": 108
    },
    {
      "key": "Behavior-1k",
      "title": "A Human-Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation",
      "tags": [
        "FeiFei",
        "benchmark",
        "OmniGibson"
      ],
      "arxiv": "https://arxiv.org/abs/2403.09227",
      "website": "https://behavior.stanford.edu/behavior-1k",
      "github": "https://github.com/StanfordVL/OmniGibson",
      "notes": "the 1,000 activities come from the results of an extensive survey on what do you want robots to do for you?",
      "x": 78.25,
      "y": 213.50000000000003,
      "zIndex": 111
    },
    {
      "key": "ARCap",
      "title": "Collecting High-quality Human Demonstrations for Robot Learning with Augmented Reality Feedback",
      "tags": [
        "FeiFei",
        "data collection"
      ],
      "arxiv": "https://arxiv.org/abs/2410.08464",
      "website": "https://stanford-tml.github.io/ARCap/",
      "github": "https://github.com/Ericcsr/ARCap",
      "notes": "a portable data collection system that provides visual feedback through augmented reality (AR) and haptic warnings to guide users in collecting high-quality demonstrations.",
      "x": 689,
      "y": 209.75000000000003,
      "zIndex": 113
    },
    {
      "key": "DigitalCousin",
      "title": "Automated Creation of Digital Cousins (ACDC) for Robust Policy Learning",
      "tags": [
        "FeiFei",
        "data synthesis"
      ],
      "arxiv": "https://arxiv.org/abs/2410.07408",
      "website": "https://digital-cousins.github.io/",
      "github": "https://github.com/cremebrule/digital-cousins",
      "notes": "ACDC can produce digital cousin scenes that preserve geometric and semantic affordances, and can be used to train policies that outperform policies trained on digital twins, achieving 90% vs. 25% under zero-shot sim-to-real transfer.",
      "x": 850,
      "y": 210.75000000000003,
      "zIndex": 114
    },
    {
      "key": "LeRobot",
      "title": "Making AI for Robotics more accessible with end-to-end learning",
      "tags": [
        "HuggingFace",
        "policy",
        "dataset"
      ],
      "github": "https://github.com/huggingface/lerobot",
      "notes": "contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.",
      "x": 403.0000000000001,
      "y": 83.5,
      "zIndex": 104
    },
    {
      "key": "Diffusion",
      "title": "Visuomotor Policy Learning via Action Diffusion",
      "tags": [
        "ShuranSong",
        "policy"
      ],
      "arxiv": "https://arxiv.org/abs/2303.04137",
      "website": "https://diffusion-policy.cs.columbia.edu/",
      "github": "https://github.com/real-stanford/diffusion_policy",
      "notes": "the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability.",
      "x": 333.25,
      "y": 277.25,
      "zIndex": 115
    },
    {
      "key": "OpenXEmbodiment",
      "title": "Robotic Learning Datasets and RT-X Models",
      "tags": [
        "DeepMind",
        "dataset",
        "collaboration"
      ],
      "arxiv": "https://arxiv.org/abs/2310.08864",
      "website": "https://robotics-transformer-x.github.io/",
      "github": "https://github.com/google-deepmind/open_x_embodiment",
      "notes": "It contains 1M+ real robot trajectories spanning 22 robot embodiments, from single robot arms to bi-manual robots and quadrupeds.",
      "x": 690.2500000000001,
      "y": 77.75000000000001,
      "zIndex": 105
    },
    {
      "key": "DROID",
      "title": "A Large-Scale In-the-Wild Robot Manipulation Dataset",
      "tags": [
        "DeepMind",
        "dataset",
        "collaboration",
        "robomimic"
      ],
      "arxiv": "https://arxiv.org/abs/2403.12945",
      "website": "https://droid-dataset.github.io/",
      "github": "https://droid-dataset.github.io/droid/",
      "notes": "DROID uses the same hardware setup across all 13 institutions. 76k demonstration trajectories or 350h of interaction data, collected across 564 scenes and 86 tasks",
      "x": 850.25,
      "y": 75.5,
      "zIndex": 106
    },
    {
      "key": "UMI",
      "title": "Universal Manipulation Interface. In-The-Wild Robot Teaching Without In-The-Wild Robots",
      "tags": [
        "ShuranSong",
        "data collection"
      ],
      "arxiv": "https://arxiv.org/abs/2402.10329",
      "website": "https://umi-gripper.github.io/",
      "github": "https://github.com/real-stanford/universal_manipulation_interface",
      "notes": "UMI employs hand-held grippers enable portable, low-cost, and information-rich data collection for challenging bimanual and dynamic manipulation demonstrations. learned policies are hardware-agnostic and deployable across multiple robot platforms.",
      "x": 691,
      "y": 274.75,
      "zIndex": 117
    },
    {
      "key": "ManiWAV",
      "title": "Learning Robot Manipulation from In-the-Wild Audio-Visual Data",
      "tags": [
        "ShuranSong",
        "policy",
        "audio-visual"
      ],
      "arxiv": "https://arxiv.org/abs/2406.19464",
      "website": "https://mani-wav.github.io/",
      "github": "https://github.com/real-stanford/maniwav",
      "notes": "Incorporating contact audio as additional source of information improves robustness and generalizability of the policy.",
      "x": 492.7500000000001,
      "y": 275.25000000000006,
      "zIndex": 116
    },
    {
      "key": "AlohaACT",
      "title": "Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware",
      "tags": [
        "ChelseaFinn",
        "data collection",
        "policy"
      ],
      "arxiv": "https://arxiv.org/abs/2304.13705",
      "website": "https://tonyzhaozh.github.io/aloha/",
      "github": "https://github.com/tonyzhaozh/aloha",
      "notes": "a low-cost system that performs end-to-end imitation learning directly from real demonstrations, collected with a custom teleoperation interface. Action Chunking with Transformers (ACT)",
      "x": 333,
      "y": 340.50000000000006,
      "zIndex": 118
    },
    {
      "key": "MobileAloha",
      "title": "Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation",
      "tags": [
        "ChelseaFinn",
        "data collection",
        "dataset"
      ],
      "arxiv": "https://arxiv.org/abs/2401.02117",
      "website": "https://mobile-aloha.github.io/",
      "github": "https://github.com/MarkFzp/mobile-aloha",
      "notes": "It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface.",
      "x": 689.7500000000001,
      "y": 345.00000000000006,
      "zIndex": 120
    },
    {
      "key": "ManiSkill",
      "title": "GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI",
      "tags": [
        "HaoSu",
        "benchmark",
        "simulation"
      ],
      "arxiv": "https://arxiv.org/abs/2410.00425",
      "website": "https://www.maniskill.ai/",
      "github": "https://github.com/haosulab/ManiSkill",
      "notes": "40+ skills/tasks with 2,000+ objects ready-to-use, millions of frames of demonstrations available, and dense reward functions.",
      "x": 31.75,
      "y": 403.50000000000006,
      "zIndex": 121
    },
    {
      "key": "TDMPC2",
      "title": "Scalable, Robust World Models for Continuous Control",
      "tags": [
        "HaoSu",
        "policy"
      ],
      "arxiv": "https://arxiv.org/abs/2310.16828",
      "website": "https://www.tdmpc2.com/",
      "github": "https://github.com/nicklashansen/tdmpc2",
      "notes": "agent capabilities increase with model and data size, and successfully train a single agent to perform 80 tasks across multiple task domains, embodiments, and action spaces",
      "x": 343.75,
      "y": 404.25000000000006,
      "zIndex": 123
    },
    {
      "key": "RFCL",
      "title": "Reverse Forward Curriculum Learning for Extreme Sample and Demonstration Efficiency in RL",
      "tags": [
        "HaoSu",
        "policy"
      ],
      "arxiv": "https://arxiv.org/abs/2405.03379",
      "website": "https://reverseforward-cl.github.io/",
      "github": "https://github.com/stonet2000/rfcl",
      "notes": "RFCL is capable of solving a wide range of complex tasks from just 1-10 demonstrations, far more demonstration efficient than prior model-free baselines.",
      "x": 493.5000000000001,
      "y": 404.25000000000006,
      "zIndex": 124
    },
    {
      "key": "SIMPLER",
      "title": "Evaluating Real-World Robot Manipulation Policies in Simulation",
      "tags": [
        "DeepMind",
        "HaoSu",
        "ChelseaFinn",
        "benchmark"
      ],
      "arxiv": "https://arxiv.org/pdf/2405.05941",
      "website": "https://simpler-env.github.io/",
      "github": "https://github.com/simpler-env/SimplerEnv",
      "notes": "demonstrate strong correlation between policy performance in SIMPLER environments and in the real world",
      "x": 178.75000000000006,
      "y": 403.50000000000006,
      "zIndex": 122
    },
    {
      "key": "OpenVLA",
      "title": "An Open-Source Vision-Language-Action Model",
      "tags": [
        "DeepMind",
        "ChelseaFinn",
        "policy"
      ],
      "arxiv": "https://arxiv.org/abs/2406.09246",
      "website": "https://openvla.github.io/",
      "github": "https://github.com/openvla/openvla",
      "notes": "a 7B parameter open-source vision-language-action model (VLA), pretrained on 970k robot episodes from the Open X-Embodiment dataset.",
      "x": 494.25,
      "y": 342.25,
      "zIndex": 119
    },
    {
      "key": "LAPA",
      "title": "Latent Action Pretraining from Videos",
      "tags": [
        "NVIDIA",
        "KAIST",
        "policy"
      ],
      "arxiv": "https://arxiv.org/abs/2410.11758",
      "website": "https://latentactionpretraining.github.io/",
      "github": "https://github.com/LatentActionPretraining/LAPA",
      "notes": "a method to learn from internet-scale videos that do not have robot action labels.",
      "x": 495.0000000000001,
      "y": 144.25,
      "zIndex": 109
    }
  ],
  "zoom": 1
}